{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcfceb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_theme(style=\"ticks\", color_codes=True)\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7316fa5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO][2023-06-06 23:28:55 - Mod: functions - Func: load_data - Line: 600]: Loaded data for bbc. 5 classes: ['sport', 'entertainment', 'tech', 'politics', 'business']. Number of documents: 2225.\n"
     ]
    }
   ],
   "source": [
    "database_name = 'bbc'\n",
    "\n",
    "data = load_data(database_name)\n",
    "data2 = SimplePreprocessing().transform(data.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "566b4640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "num_w = [len(re.findall(r'\\w+', sentence)) for sentence in data2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b202b9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "197.07280898876405"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(num_w)/(len(data2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329f5d40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff6d2b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bert1(database_name):\n",
    "    \n",
    "    try: \n",
    "        database = load_data(database_name=database_name)\n",
    "        \n",
    "        x_train = list(compress(database.data, database.is_train==1))\n",
    "        y_train = list(compress(database.target, database.is_train==1))\n",
    "        x_test = list(compress(database.data, database.is_train==0))\n",
    "        y_test = list(compress(database.target, database.is_train==0))\n",
    "\n",
    "        (x_train,  y_train), (x_test, y_test), preproc = text.texts_from_array(\n",
    "            x_train,\n",
    "            y_train,\n",
    "            x_test, \n",
    "            y_test,\n",
    "            class_names=database.target_names,\n",
    "            preprocess_mode='bert',\n",
    "            maxlen=350, \n",
    "            max_features=35000)\n",
    "        \n",
    "        database_name = 'classic4'\n",
    "        path = f\"./pickle_objects/models/model_bert_{database_name}.pickle\"\n",
    "        if os.path.isfile(path):            \n",
    "            with open(path, 'rb') as f:\n",
    "                learner = pickle.load(f)  \n",
    "            logger.info(f'Load pre trained BERT model for dataset {database_name}.')    \n",
    "            \n",
    "        else:  \n",
    "            print('oi')\n",
    "#             bert_model = text.text_classifier(\"bert\", train_data=(x_train, y_train),\n",
    "#                                               preproc=preproc)\n",
    "#             learner = ktrain.get_learner(bert_model, train_data=(x_train, y_train), \n",
    "#                                             batch_size=6)\n",
    "#             learner.fit_onecycle(2e-5, 4)\n",
    "\n",
    "#             with open(f\"./pickle_objects/models/model_bert_{database_name}.pickle\", \"wb\") as f:\n",
    "#                 pickle.dump(learner, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "            #logger.info(f'Fitted BERT model on dataset {database_name}.')\n",
    "        \n",
    "        conf_table = learner.validate(val_data=(x_test, y_test),\n",
    "                            class_names=database.target_names)\n",
    "        \n",
    "        return conf_table\n",
    "  \n",
    "    except Exception as e:\n",
    "        logger.info(f'Error fitting BERT for {database_name}: \\n {e}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e4210b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2184b90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f3c967",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./pickle_objects/preprocess/tpbg/20ng/heterodata_pbg_20ng_tpbg_docf_None_train.pickle', 'rb') as f:\n",
    "    heterodata_train = pickle.load(f)\n",
    "with open(f'./pickle_objects/preprocess/tpbg/20ng/heterodata_pbg_20ng_tpbg_docf_None_test.pickle', 'rb') as f:\n",
    "    heterodata_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c541c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HeteroGNN(metadata=heterodata_train.metadata(), \n",
    "                  hidden_channels=100, \n",
    "                  out_channels=heterodata_train['source']['num_classes'],\n",
    "                  num_layers=2,\n",
    "                  p_dropout=0,\n",
    "                  aggr='sum', \n",
    "                  version=2)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4d6ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "heterodata_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983b1f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "\n",
    "model.train()\n",
    "optimizer.zero_grad()\n",
    "out = model(heterodata_train.x_dict, heterodata_train.edge_index_dict)\n",
    "prediction = out.argmax(1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2136d746",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "database_name='webkb'\n",
    "K=100\n",
    "disable_tqdm=True\n",
    "\n",
    "try:\n",
    "    database_train = load_data(database_name=database_name, subset='train')\n",
    "    database_test = load_data(database_name=database_name, subset='test')\n",
    "\n",
    "    data_preprocessed_train = SimplePreprocessing().transform(database_train.data)\n",
    "    data_preprocessed_test = SimplePreprocessing().transform(database_test.data)\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    data_vectorized_train_fit = vectorizer.fit_transform(data_preprocessed_train)\n",
    "    data_vectorized_test = vectorizer.transform(data_preprocessed_test)\n",
    "\n",
    "    pbg_train = UPBG(K, alpha=0.005, beta=0.001, local_max_itr=50, global_max_itr=10,\n",
    "       local_threshold=1e-6, global_threshold=1e-6,\n",
    "       feature_names=vectorizer.get_feature_names_out(), disable_tqdm=disable_tqdm)\n",
    "    pbg_train.fit(data_vectorized_train_fit, database_train.target)\n",
    "\n",
    "    predicted_target = pbg_train.predict(data_vectorized_test)\n",
    "    micro_train = f1_score(predicted_target, database_test.target, average='micro')\n",
    "    logger.info(f'Performance on pretrained pbg on {database_name} with K={K}. F1: {micro_train:.4f}.')\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    data_vectorized_test_fit = vectorizer.fit_transform(data_preprocessed_test)\n",
    "    data_vectorized_test = vectorizer.transform(data_preprocessed_test)\n",
    "    pbg_test = UPBG(K, alpha=0.005, beta=0.001, local_max_itr=50, global_max_itr=10,\n",
    "           local_threshold=1e-6, global_threshold=1e-6,\n",
    "           feature_names=vectorizer.get_feature_names_out(), disable_tqdm=disable_tqdm)\n",
    "    pbg_test.fit(data_vectorized_test_fit, predicted_target)\n",
    "    y_pred = pbg_test.predict(data_vectorized_test)\n",
    "    micro_test = f1_score(y_pred, database_test.target, average='micro')\n",
    "\n",
    "    logger.info(f'Loaded pbg for {database_name} with K={K}. F1: {micro_test:.4f}.')\n",
    "    #return pbg_train, pbg_test\n",
    "\n",
    "except Exception as e:\n",
    "    logger.info(f'Error fitting pbg for {database_name}: \\n {e}')    \n",
    "\n",
    "#alpha=0.05, beta=0.0001, local_max_itr=5,\n",
    "#global_max_itr=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e365f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbg_train = UPBG(K, alpha=0.0005, beta=0.001, local_max_itr=50, global_max_itr=10,\n",
    "   local_threshold=1e-6, global_threshold=1e-6,\n",
    "   feature_names=vectorizer.get_feature_names_out(), disable_tqdm=False)\n",
    "pbg_train.fit(data_vectorized_train_fit, database_train.target)\n",
    "\n",
    "predicted_target = pbg_train.predict(data_vectorized_test)\n",
    "micro_train = f1_score(predicted_target, database_test.target, average='micro')\n",
    "logger.info(f'Performance on pretrained pbg on {database_name} with K={K}. F1: {micro_train:.4f}.')\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "data_vectorized_test_fit = vectorizer.fit_transform(data_preprocessed_test)\n",
    "data_vectorized_test = vectorizer.transform(data_preprocessed_test)\n",
    "pbg_test = UPBG(K, alpha=0.0005, beta=0.001, local_max_itr=50, global_max_itr=10,\n",
    "       local_threshold=1e-6, global_threshold=1e-6,\n",
    "       feature_names=vectorizer.get_feature_names_out(), disable_tqdm=False)\n",
    "pbg_test.fit(data_vectorized_test_fit, predicted_target)\n",
    "y_pred = pbg_test.predict(data_vectorized_test)\n",
    "micro_test = f1_score(y_pred, database_test.target, average='micro')\n",
    "\n",
    "logger.info(f'Loaded pbg for {database_name} with K={K}. F1: {micro_test:.4f}.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc5d60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbg_test = UPBG(K, alpha=0.005, beta=0.001, local_max_itr=50, global_max_itr=10,\n",
    "       local_threshold=1e-6, global_threshold=1e-6,\n",
    "       feature_names=vectorizer.get_feature_names_out(), disable_tqdm=disable_tqdm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c189d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbg_test.fit(data_vectorized_test_fit, predicted_target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a5105e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vectorized_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0f6f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_pbg_test(database_name='nsf', pbg_model_trained=pbg_train, K=200, disable_tqdm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48ebe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pbg_test.predict(data_vectorized_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7633a8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "micro_test = f1_score(y_pred, database_test.target, average='micro')\n",
    "\n",
    "logger.info(f'Loaded pbg for {database_name} with K={K}. F1: {micro_test:.4f}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186f2c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of documents NFS, Ohscal, WebKB, Industry Sector e MultiDomainSent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08059c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of terms (WebKB, Reviews, Industry Sector, Review Polarity e Hitech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83018e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import webtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed01d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "database_name = '20newsgroups'\n",
    "\n",
    "pipe_lda = get_lda_train(database_name=database_name, K=100)\n",
    "micro_lda = test_pipe(pipe_lda, database_name=database_name)\n",
    "print(f'{database_name} - micro_lda: {micro_lda}')\n",
    "\n",
    "\n",
    "pipe_nmf = get_nmf_train(database_name=database_name, K=100)\n",
    "micro_nmf = test_pipe(pipe_nmf, database_name=database_name)\n",
    "print(f'{database_name} - micro_nmf: {micro_nmf}')\n",
    "\n",
    "#nsf - micro_lda: 0.9498721227621484\n",
    "#nsf - micro_nmf: 0.9989769820971867\n",
    "\n",
    "#classic4 - micro_lda: 0.9996476391825229\n",
    "#classic4 - micro_nmf: 0.9992952783650458\n",
    "\n",
    "#webkb - micro_lda: 0.9975852701479022\n",
    "#webkb - micro_nmf: 0.9981889526109267"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4276be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67a83b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### CARREGANDO DADOS  #######################\n",
    "\n",
    "data_train = load_data(database_name='reuters', subset='train')\n",
    "data_test = load_data(database_name='reuters', subset='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f686d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = load_data(database_name='nsf', subset='train')\n",
    "data_test = load_data(database_name='nsf', subset='test')\n",
    "\n",
    "print('preprocessing...')\n",
    "pp = SimplePreprocessing()\n",
    "M_train = pp.transform(data_train.data)\n",
    "M_test = pp.transform(data_test.data)\n",
    "\n",
    "print('done.')\n",
    "vectorizer = TfidfVectorizer() #ngram_range=(1, 3)\n",
    "M_train = vectorizer.fit_transform(M_train)\n",
    "M_test = vectorizer.transform(M_test)\n",
    "\n",
    "### PBG\n",
    "K=100\n",
    "pbg = UPBG(K, alpha=0.005, beta=0.001, local_max_itr=50, global_max_itr=10,\n",
    "           local_threshold=1e-6, global_threshold=1e-6,\n",
    "           feature_names=vectorizer.get_feature_names_out())\n",
    "\n",
    "#pbg = MultinomialNB(alpha=0.1)\n",
    "#pbg = SVC()\n",
    "\n",
    "print(\"fitting...\")\n",
    "pbg.fit(M_train, data_train.target)\n",
    "print('done')\n",
    "\n",
    "\n",
    "y_pred = pbg.predict(M_test)\n",
    "\n",
    "score = f1_score(y_pred, data_test.target, average='micro')\n",
    "print(f'f1 score: {score}')\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680db71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pbg(database_name, K=100, disable_tqdm=True):\n",
    "    \n",
    "    try:\n",
    "        database_train = load_data(database_name=database_name, subset='train')\n",
    "        database_test = load_data(database_name=database_name, subset='test')\n",
    "\n",
    "        data_preprocessed_train = SimplePreprocessing().transform(database_train.data)\n",
    "        data_preprocessed_test = SimplePreprocessing().transform(database_test.data)\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        data_vectorized_train_fit = vectorizer.fit_transform(data_preprocessed_train)\n",
    "        data_vectorized_test = vectorizer.transform(data_preprocessed_test)\n",
    "        \n",
    "        pbg_train = UPBG(K, alpha=0.005, beta=0.001, local_max_itr=50, global_max_itr=10,\n",
    "           local_threshold=1e-6, global_threshold=1e-6,\n",
    "           feature_names=vectorizer.get_feature_names_out(), disable_tqdm=disable_tqdm)\n",
    "        pbg_train.fit(data_vectorized_train_fit, database_train.target)\n",
    "        \n",
    "        predicted_target = pbg_train.predict(data_vectorized_test)\n",
    "        micro_train = f1_score(predicted_target, database_test.target, average='micro')\n",
    "        logger.info(f'Performance on pretrained pbg on {database_name} with K={K}. F1: {micro_train:.4f}.')\n",
    "        \n",
    "        vectorizer = TfidfVectorizer()\n",
    "        data_vectorized_test_fit = vectorizer.fit_transform(data_preprocessed_test)\n",
    "        data_vectorized_test = vectorizer.transform(data_preprocessed_test)\n",
    "        pbg_test = UPBG(K, alpha=0.005, beta=0.001, local_max_itr=50, global_max_itr=10,\n",
    "               local_threshold=1e-6, global_threshold=1e-6,\n",
    "               feature_names=vectorizer.get_feature_names_out(), disable_tqdm=disable_tqdm)\n",
    "        pbg_test.fit(data_vectorized_test_fit, predicted_target)\n",
    "        y_pred = pbg_test.predict(data_vectorized_test)\n",
    "        micro_test = f1_score(y_pred, database_test.target, average='micro')\n",
    "\n",
    "        logger.info(f'Loaded pbg for {database_name} with K={K}. F1: {micro_test:.4f}.')\n",
    "        return pbg_train, pbg_test\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.info(f'Error fitting pbg for {database_name}: \\n {e}')\n",
    "\n",
    "# pbg_train, pbg_test = run_pbg(database_name='nsf', K=100, disable_tqdm=True)\n",
    "# heterodata_pbg_nsf_k100_train = get_heterograph_pbg(pbg_train)\n",
    "# heterodata_pbg_nsf_k100_test_full = get_heterograph_pbg(pbg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdba73a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbg_train, pbg_test = run_pbg(database_name='20newsgroups', K=100, disable_tqdm=True)\n",
    "heterodata_pbg_nsf_k200_train = get_heterograph_pbg(pbg_train)\n",
    "heterodata_pbg_nsf_k200_test_full = get_heterograph_pbg(pbg_test)\n",
    "\n",
    "heterodata_pbg_nsf_k200_val, heterodata_pbg_nsf_k200_test, heterodata_pbg_nsf_k200_score = split_heterodata(heterodata_pbg_nsf_k200_test_full)\n",
    "\n",
    "\n",
    "hidden_channels_list=[10, 50, 100, 200]\n",
    "num_layers_list=[2, 3, 4]\n",
    "p_dropout_list=[0.2]\n",
    "patience=10\n",
    "\n",
    "logger.info(\"Running experiments on nsf K=200\")\n",
    "df_experiment_nsf_k100 = experiment_gnn(database_name='nsf K=200',\n",
    "                        heterodata_pbg_train=heterodata_pbg_nsf_k200_train,\n",
    "                        heterodata_pbg_val=heterodata_pbg_nsf_k200_val,\n",
    "                        heterodata_pbg_test=heterodata_pbg_nsf_k200_test,\n",
    "                        hidden_channels_list=hidden_channels_list,\n",
    "                        num_layers_list=num_layers_list,\n",
    "                        p_dropout_list=p_dropout_list,\n",
    "                        patience=patience,\n",
    "                        verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d89528d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = experiment_gnn(database_name='nsf K=200',\n",
    "                        heterodata_pbg_train=heterodata_pbg_nsf_k200_train,\n",
    "                        heterodata_pbg_val=heterodata_pbg_nsf_k200_val,\n",
    "                        heterodata_pbg_test=heterodata_pbg_nsf_k200_test,\n",
    "                        hidden_channels_list=hidden_channels_list,\n",
    "                        num_layers_list=num_layers_list,\n",
    "                        p_dropout_list=p_dropout_list,\n",
    "                        patience=100,\n",
    "                        verbose=False)\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0186a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d244fc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d2158b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_experiment_nsf_k100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9957bc02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1e453f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8af2ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Running experiments on nsf K=200\")\n",
    "df_experiment_nsf_k200 = experiment_gnn(database_name='nsf K=200',\n",
    "                        heterodata_pbg_train=heterodata_pbg_nsf_k200_train,\n",
    "                        heterodata_pbg_val=heterodata_pbg_nsf_k200_val,\n",
    "                        heterodata_pbg_test=heterodata_pbg_nsf_k200_test,\n",
    "                        hidden_channels_list=hidden_channels_list,\n",
    "                        num_layers_list=num_layers_list,\n",
    "                        p_dropout_list=p_dropout_list,\n",
    "                        patience=patience,\n",
    "                        verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e226ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "heterodata_pbg_nsf_k100_val, heterodata_pbg_nsf_k100_test, heterodata_pbg_nsf_k100_score = split_heterodata(heterodata_pbg_nsf_k100_test_full)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096668d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_channels_list=[10, 50, 100, 200]\n",
    "num_layers_list=[2, 3, 4]\n",
    "p_dropout_list=[0.2]\n",
    "patience=100\n",
    "\n",
    "logger.info(\"Running experiments on nsf K=100\")\n",
    "df_experiment_nsf_k100 = experiment_gnn(database_name='nsf K=100',\n",
    "                        heterodata_pbg_train=heterodata_pbg_nsf_k100_train,\n",
    "                        heterodata_pbg_val=heterodata_pbg_nsf_k100_val,\n",
    "                        heterodata_pbg_test=heterodata_pbg_nsf_k100_test,\n",
    "                        hidden_channels_list=hidden_channels_list,\n",
    "                        num_layers_list=num_layers_list,\n",
    "                        p_dropout_list=p_dropout_list,\n",
    "                        patience=patience,\n",
    "                        verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fcf80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_experiment_nsf_k100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d4346c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d66fdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbg = UPBG(K, alpha=0.005, beta=0.001, local_max_itr=50, global_max_itr=10,\n",
    "           local_threshold=1e-6, global_threshold=1e-6,\n",
    "           feature_names=vectorizer.get_feature_names_out())\n",
    "\n",
    "#pbg = MultinomialNB(alpha=0.1)\n",
    "#pbg = SVC()\n",
    "\n",
    "print(\"fitting...\")\n",
    "pbg.fit(M_train, data_train.target)\n",
    "print('done')\n",
    "\n",
    "\n",
    "y_pred = pbg.predict(M_test)\n",
    "\n",
    "score = f1_score(y_pred, data_test.target, average='micro')\n",
    "print(f'f1 score: {score}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fdf21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.svm import SVC\n",
    "from util import SimplePreprocessing\n",
    "from upbg import UPBG\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "data_train = load_data(database_name='reuters', subset='train')\n",
    "data_test = load_data(database_name='reuters', subset='test')\n",
    "\n",
    "print('preprocessing...')\n",
    "pp = SimplePreprocessing()\n",
    "M_train = pp.transform(data_train.data)\n",
    "M_test = pp.transform(data_test.data)\n",
    "\n",
    "print('done.')\n",
    "vectorizer = TfidfVectorizer() #ngram_range=(1, 3)\n",
    "M_train = vectorizer.fit_transform(M_train)\n",
    "M_test = vectorizer.transform(M_test)\n",
    "\n",
    "d = {'alt': 'alternative', 'comp': 'computer', 'os': 'operation system', 'ms-windows': 'windows', 'sys': 'system', 'x': 'interface',\n",
    "     'misc': 'miscellaneous', 'rec': 'recreation', 'autos': 'automobile', 'sci': 'science', 'crypt': 'cryptography', 'med': 'medicine', 'soc': 'society'}\n",
    "cls_names_ext = [[d.get(s, s) for s in name.split('.')]\n",
    "                 for name in newsgroups_train.target_names]     \n",
    "\n",
    "print(cls_names_ext)\n",
    "\n",
    "\n",
    "### PBG\n",
    "n_class = len(set(newsgroups_train.target))\n",
    "print(f'nclass {n_class}')\n",
    "K=100\n",
    "pbg = UPBG(K, alpha=0.005, beta=0.001, local_max_itr=50, global_max_itr=10,\n",
    "           local_threshold=1e-6, global_threshold=1e-6,\n",
    "           feature_names=vectorizer.get_feature_names())\n",
    "\n",
    "#pbg = MultinomialNB(alpha=0.1)\n",
    "#pbg = SVC()\n",
    "\n",
    "print(\"fitting...\")\n",
    "pbg.fit(M_train, newsgroups_train.target)\n",
    "print('done')\n",
    "\n",
    "pbg.print_top_topics(n_top_words=5, target_name=newsgroups_train.target_names)\n",
    "\n",
    "y_pred = pbg.predict(M_test)\n",
    "\n",
    "score = f1_score(y_pred, newsgroups_test.target, average='micro')\n",
    "print(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d7dca9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd773af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERTopic\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "data = fetch_20newsgroups(subset='all')['data']\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "embeddings = model.encode(data, show_progress_bar=True)\n",
    "\n",
    "\n",
    "from umap.umap_ import UMAP\n",
    "umap_embeddings = UMAP(n_neighbors=15, \n",
    "                            n_components=5, \n",
    "                            metric='cosine').fit_transform(embeddings)\n",
    "\n",
    "\n",
    "import hdbscan\n",
    "cluster = hdbscan.HDBSCAN(min_cluster_size=15,\n",
    "                          metric='euclidean',                      \n",
    "                          cluster_selection_method='eom').fit(umap_embeddings)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare data\n",
    "umap_data = umap.UMAP(n_neighbors=15, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)\n",
    "result = pd.DataFrame(umap_data, columns=['x', 'y'])\n",
    "result['labels'] = cluster.labels_\n",
    "\n",
    "# Visualize clusters\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "outliers = result.loc[result.labels == -1, :]\n",
    "clustered = result.loc[result.labels != -1, :]\n",
    "plt.scatter(outliers.x, outliers.y, color='#BDBDBD', s=0.05)\n",
    "plt.scatter(clustered.x, clustered.y, c=clustered.labels, s=0.05, cmap='hsv_r')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2dbc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df = pd.DataFrame(data, columns=[\"Doc\"])\n",
    "docs_df['Topic'] = cluster.labels_\n",
    "docs_df['Doc_ID'] = range(len(docs_df))\n",
    "docs_per_topic = docs_df.groupby(['Topic'], as_index = False).agg({'Doc': ' '.join})\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def c_tf_idf(documents, m, ngram_range=(1, 1)):\n",
    "    count = CountVectorizer(ngram_range=ngram_range, stop_words=\"english\").fit(documents)\n",
    "    t = count.transform(documents).toarray()\n",
    "    w = t.sum(axis=1)\n",
    "    tf = np.divide(t.T, w)\n",
    "    sum_t = t.sum(axis=0)\n",
    "    idf = np.log(np.divide(m, sum_t)).reshape(-1, 1)\n",
    "    tf_idf = np.multiply(tf, idf)\n",
    "\n",
    "    return tf_idf, count\n",
    "  \n",
    "tf_idf, count = c_tf_idf(docs_per_topic.Doc.values, m=len(data))\n",
    "\n",
    "def extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20):\n",
    "    words = count.get_feature_names()\n",
    "    labels = list(docs_per_topic.Topic)\n",
    "    tf_idf_transposed = tf_idf.T\n",
    "    indices = tf_idf_transposed.argsort()[:, -n:]\n",
    "    top_n_words = {label: [(words[j], tf_idf_transposed[i][j]) for j in indices[i]][::-1] for i, label in enumerate(labels)}\n",
    "    return top_n_words\n",
    "\n",
    "def extract_topic_sizes(df):\n",
    "    topic_sizes = (df.groupby(['Topic'])\n",
    "                     .Doc\n",
    "                     .count()\n",
    "                     .reset_index()\n",
    "                     .rename({\"Topic\": \"Topic\", \"Doc\": \"Size\"}, axis='columns')\n",
    "                     .sort_values(\"Size\", ascending=False))\n",
    "    return topic_sizes\n",
    "\n",
    "top_n_words = extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20)\n",
    "topic_sizes = extract_topic_sizes(docs_df); topic_sizes.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72f1d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "news20 = fetch_20newsgroups()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    news20.data[:5000], news20.target[:5000],\n",
    "    stratify=news20.target[:5000])\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db40f1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "database = load_data(database_name=\"20newsgroups\", subset=\"train\")\n",
    "\n",
    "database.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75791d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer as CV\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def main():\n",
    "    news20 = fetch_20newsgroups()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        news20.data[:5000], news20.target[:5000],\n",
    "        stratify=news20.target[:5000])\n",
    "    CountVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "    cv = CV(min_df=0.04, stop_words=\"english\")\n",
    "    lda = LDA(n_components=100, max_iter=30, n_jobs=-1)\n",
    "    rfc = RFC(n_estimators=500, n_jobs=-1)\n",
    "    estimators = [(\"cv\", cv), (\"lda\", lda), (\"rfc\", rfc)]\n",
    "    pl = Pipeline(estimators)\n",
    "\n",
    "    pl.fit(X_train, y_train)\n",
    "    y_pred = pl.predict(X_test)\n",
    "    print(classification_report(\n",
    "        y_test, y_pred, target_names=news20.target_names))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebe22a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall umap -Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bc05d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ktrain\n",
    "from ktrain import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd50bcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bert(database_name):\n",
    "    \n",
    "    try: \n",
    "        database_train = load_data(database_name=database_name, subset=\"train\")\n",
    "        database_test = load_data(database_name=database_name, subset=\"test\")\n",
    "        \n",
    "        (x_train,  y_train), (x_test, y_test), preproc = text.texts_from_array(\n",
    "            x_train=database_train.data,\n",
    "            y_train=database_train.target,\n",
    "            x_test=database_test.data, \n",
    "            y_test=database_test.target,\n",
    "            class_names=database.target_names,\n",
    "            preprocess_mode='bert',\n",
    "            maxlen=350, \n",
    "            max_features=35000)\n",
    "        \n",
    "        bert_model = text.text_classifier(\"bert\", train_data=(x_train, y_train),\n",
    "                                          preproc=preproc)\n",
    "        learner = ktrain.get_learner(bert_model, train_data=(x_train, y_train), \n",
    "                                        batch_size=6)\n",
    "        learner.fit_onecycle(2e-5, 4)\n",
    "        \n",
    "        with open(f\"./pickle_objects/model_bert_{database_name}.pickle\", \"wb\") as f:\n",
    "            pickle.dump(learner, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "        logger.info(f'Fitted BERT model on dataset {database_name}.')\n",
    "        conf_table = learner.validate(val_data=(x_test, y_test),\n",
    "                            class_names=database_train.target_names)\n",
    "        \n",
    "        return conf_table\n",
    "  \n",
    "    except Exception as e:\n",
    "        logger.info(f'Error fitting pbg for {database_name}: \\n {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d9a427",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_bert(database_name='reuters')\n",
    "load_bert(database_name='bbc_news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001a09b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "database_train = load_data(database_name=\"20newsgroups\", subset=\"train\")\n",
    "database_test = load_data(database_name=\"20newsgroups\", subset=\"test\")\n",
    "\n",
    "(x_train,  y_train), (x_test, y_test), preproc = text.texts_from_array(\n",
    "    x_train=database_train.data,\n",
    "    y_train=database_train.target,\n",
    "    x_test=database_train.data, \n",
    "    y_test=database_train.target,\n",
    "    class_names=database_train.target_names,\n",
    "    preprocess_mode='bert',\n",
    "    maxlen=350, \n",
    "    max_features=35000)\n",
    "\n",
    "bert_model = text.text_classifier(\"bert\", train_data=(x_train, y_train),\n",
    "                                  preproc=preproc)\n",
    "learner = ktrain.get_learner(bert_model, train_data=(x_train, y_train), \n",
    "                                batch_size=6)\n",
    "learner.fit_onecycle(2e-5, 4)\n",
    "\n",
    "logger.info(f'Fitted BERT model on dataset {database_name}.')\n",
    "x = learner.validate(val_data=(x_test, y_test), class_names=train_b.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff49c1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = learner.validate(val_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2591de",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c69fed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b003f411",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://towardsdatascience.com/topic-modeling-with-bert-779f7db187e6\n",
    "    \n",
    "    \n",
    "    https://maartengr.github.io/BERTopic/getting_started/topicsperclass/topicsperclass.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
