{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCC5890 Redes Neurais\n",
    "### Exercício prático 2\n",
    "### Equipe:\n",
    " * Bruno F. Bessa 5881890\n",
    " * Leonardo Almeida 5834097\n",
    " * Khennedy Bacule 12619430"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCICIO 2\n",
    "\n",
    "- Implemente uma rede MLP usando apenas pacotes básicos do Python, como a biblioteca Numpy.\n",
    "\n",
    "- Trabalhos que fizerem uso de bibliotecas prontas para redes neurais, como Tensorflow, Keras, Sklearn ou outros similares, perderá 50% na nota.\n",
    "\n",
    "I - Experimente a rede implementada no problema do XOR.\n",
    "\n",
    "II - Considere o problema de auto-associador (encoding problem) no qual um conjunto de padrões ortogonais de entrada são mapeados num conjunto de padrões de saída ortogonais através de uma camada oculta com um número pequeno de neurônios. A figura acima mostra a arquitetura básica para se resolver este problema.\n",
    "\n",
    "- Essencialmente, o problema é aprender uma codificação de padrão com p-bit em um padrão de log2 p-bit, e em seguida aprender a decodificar esta representação num padrão de saída.\n",
    "\n",
    "- Pede-se: Construir o mapeamento gerado por uma rede multi-camadas com o algoritmo backpropagation (BP), para o caso do mapeamento identidade, considerando dois casos: \n",
    "\n",
    "a) Padrão de entrada e Padrão de Saída: Id(8x8) e Id(8X8) \n",
    "\n",
    "b) Padrão de entrada e Padrão de Saída: Id(15x15) e Id(15X15) \n",
    "\n",
    "- Onde Id denota a matriz identidade. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import random\n",
    "\n",
    "\n",
    "class MLP(object):\n",
    "    \"\"\"A Multilayer Perceptron class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_inputs=3, hidden_layers=[3, 3], num_outputs=2):\n",
    "        \"\"\"Constructor for the MLP. Takes the number of inputs,\n",
    "            a variable number of hidden layers, and number of outputs\n",
    "        Args:\n",
    "            num_inputs (int): Number of inputs\n",
    "            hidden_layers (list): A list of ints for the hidden layers\n",
    "            num_outputs (int): Number of outputs\n",
    "        \"\"\"\n",
    "\n",
    "        self.num_inputs = num_inputs\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.num_outputs = num_outputs\n",
    "\n",
    "        # create a generic representation of the layers\n",
    "        layers = [num_inputs] + hidden_layers + [num_outputs]\n",
    "\n",
    "        # create random connection weights for the layers\n",
    "        weights = []\n",
    "        for i in range(len(layers) - 1):\n",
    "            w = np.random.rand(layers[i], layers[i + 1])\n",
    "            weights.append(w)\n",
    "        self.weights = weights\n",
    "\n",
    "        # save derivatives per layer\n",
    "        derivatives = []\n",
    "        for i in range(len(layers) - 1):\n",
    "            d = np.zeros((layers[i], layers[i + 1]))\n",
    "            derivatives.append(d)\n",
    "        self.derivatives = derivatives\n",
    "\n",
    "        # save activations per layer\n",
    "        activations = []\n",
    "        for i in range(len(layers)):\n",
    "            a = np.zeros(layers[i])\n",
    "            activations.append(a)\n",
    "        self.activations = activations\n",
    "\n",
    "\n",
    "    def forward_propagate(self, inputs):\n",
    "        \"\"\"Computes forward propagation of the network based on input signals.\n",
    "        Args:\n",
    "            inputs (ndarray): Input signals\n",
    "        Returns:\n",
    "            activations (ndarray): Output values\n",
    "        \"\"\"\n",
    "\n",
    "        # the input layer activation is just the input itself\n",
    "        activations = inputs\n",
    "\n",
    "        # save the activations for backpropogation\n",
    "        self.activations[0] = activations\n",
    "\n",
    "        # iterate through the network layers\n",
    "        for i, w in enumerate(self.weights):\n",
    "            # calculate matrix multiplication between previous activation and weight matrix\n",
    "            net_inputs = np.dot(activations, w)\n",
    "\n",
    "            # apply sigmoid activation function\n",
    "            activations = self._sigmoid(net_inputs)\n",
    "\n",
    "            # save the activations for backpropogation\n",
    "            self.activations[i + 1] = activations\n",
    "\n",
    "        # return output layer activation\n",
    "        return activations\n",
    "\n",
    "\n",
    "    def back_propagate(self, error):\n",
    "        \"\"\"Backpropogates an error signal.\n",
    "        Args:\n",
    "            error (ndarray): The error to backprop.\n",
    "        Returns:\n",
    "            error (ndarray): The final error of the input\n",
    "        \"\"\"\n",
    "\n",
    "        # iterate backwards through the network layers\n",
    "        for i in reversed(range(len(self.derivatives))):\n",
    "\n",
    "            # get activation for previous layer\n",
    "            activations = self.activations[i+1]\n",
    "\n",
    "            # apply sigmoid derivative function\n",
    "            delta = error * self._sigmoid_derivative(activations)\n",
    "\n",
    "            # reshape delta as to have it as a 2d array\n",
    "            delta_re = delta.reshape(delta.shape[0], -1).T\n",
    "\n",
    "            # get activations for current layer\n",
    "            current_activations = self.activations[i]\n",
    "\n",
    "            # reshape activations as to have them as a 2d column matrix\n",
    "            current_activations = current_activations.reshape(current_activations.shape[0],-1)\n",
    "\n",
    "            # save derivative after applying matrix multiplication\n",
    "            self.derivatives[i] = np.dot(current_activations, delta_re)\n",
    "\n",
    "            # backpropogate the next error\n",
    "            error = np.dot(delta, self.weights[i].T)\n",
    "\n",
    "\n",
    "    def train(self, inputs, targets, epochs, learning_rate, verbose=False):\n",
    "        \"\"\"Trains model running forward prop and backprop\n",
    "        Args:\n",
    "            inputs (ndarray): X\n",
    "            targets (ndarray): Y\n",
    "            epochs (int): Num. epochs we want to train the network for\n",
    "            learning_rate (float): Step to apply to gradient descent\n",
    "        \"\"\"\n",
    "        # now enter the training loop\n",
    "        for i in range(epochs):\n",
    "            sum_errors = 0\n",
    "\n",
    "            # iterate through all the training data\n",
    "            for j, input in enumerate(inputs):\n",
    "                target = targets[j]\n",
    "\n",
    "                # activate the network!\n",
    "                output = self.forward_propagate(input)\n",
    "\n",
    "                error = target - output\n",
    "\n",
    "                self.back_propagate(error)\n",
    "\n",
    "                # now perform gradient descent on the derivatives\n",
    "                # (this will update the weights\n",
    "                self.gradient_descent(learning_rate)\n",
    "\n",
    "                # keep track of the MSE for reporting later\n",
    "                sum_errors += self._mse(target, output)\n",
    "\n",
    "            if verbose:\n",
    "                # Epoch complete, report the training error\n",
    "                print(\"Error: {} at epoch {}\".format(sum_errors / len(items), i+1))\n",
    "\n",
    "        print(\"Training complete!\")\n",
    "        \n",
    "\n",
    "    def gradient_descent(self, learningRate=1):\n",
    "        \"\"\"Learns by descending the gradient\n",
    "        Args:\n",
    "            learningRate (float): How fast to learn.\n",
    "        \"\"\"\n",
    "        # update the weights by stepping down the gradient\n",
    "        for i in range(len(self.weights)):\n",
    "            weights = self.weights[i]\n",
    "            derivatives = self.derivatives[i]\n",
    "            weights += derivatives * learningRate\n",
    "\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        \"\"\"Sigmoid activation function\n",
    "        Args:\n",
    "            x (float): Value to be processed\n",
    "        Returns:\n",
    "            y (float): Output\n",
    "        \"\"\"\n",
    "\n",
    "        y = 1.0 / (1 + np.exp(-x))\n",
    "        return y\n",
    "\n",
    "\n",
    "    def _sigmoid_derivative(self, x):\n",
    "        \"\"\"Sigmoid derivative function\n",
    "        Args:\n",
    "            x (float): Value to be processed\n",
    "        Returns:\n",
    "            y (float): Output\n",
    "        \"\"\"\n",
    "        return x * (1.0 - x)\n",
    "\n",
    "\n",
    "    def _mse(self, target, output):\n",
    "        \"\"\"Mean Squared Error loss function\n",
    "        Args:\n",
    "            target (ndarray): The ground trut\n",
    "            output (ndarray): The predicted values\n",
    "        Returns:\n",
    "            (float): Output\n",
    "        \"\"\"\n",
    "        return np.average((target - output) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n",
      "input:  [1. 1.]  output:  [1.]\n",
      "input:  [0. 0.]  output:  [1.]\n",
      "input:  [0. 1.]  output:  [1.]\n",
      "input:  [1. 0.]  output:  [1.]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # A) XOR\n",
    "    mlp = MLP(num_inputs=2, hidden_layers=[1], num_outputs=1)\n",
    "    x = np.array([[1.0, 1.0],\n",
    "                  [0.0, 0.0],\n",
    "                  [0.0, 1.0],\n",
    "                  [1.0, 0.0]])\n",
    "    y = np.array([[0.0], [0.0], [1.0], [1.0]])\n",
    "\n",
    "    # train network\n",
    "    mlp.train(inputs=x, targets=y, epochs=50, learning_rate=0.1)\n",
    "\n",
    "    # validade model\n",
    "    for row in x:\n",
    "        input = np.array(row)\n",
    "        output = np.round(mlp.forward_propagate(input), 0)\n",
    "        print(\"input: \", input, \" output: \", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix id(8): \n",
      " [[1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Training complete!\n",
      "input:  [1. 0. 0. 0. 0. 0. 0. 0.]  output:  [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "input:  [0. 1. 0. 0. 0. 0. 0. 0.]  output:  [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "input:  [0. 0. 1. 0. 0. 0. 0. 0.]  output:  [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "input:  [0. 0. 0. 1. 0. 0. 0. 0.]  output:  [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "input:  [0. 0. 0. 0. 1. 0. 0. 0.]  output:  [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "input:  [0. 0. 0. 0. 0. 1. 0. 0.]  output:  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "input:  [0. 0. 0. 0. 0. 0. 1. 0.]  output:  [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "input:  [0. 0. 0. 0. 0. 0. 0. 1.]  output:  [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "Matrix: Id(15)\n",
      " [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Training complete!\n",
      "input:  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]  output:  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "input:  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]  output:  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "input:  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]  output:  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "input:  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]  output:  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "input:  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]  output:  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "input:  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]  output:  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "input:  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]  output:  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "input:  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]  output:  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "input:  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]  output:  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "input:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]  output:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "input:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]  output:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "input:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]  output:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "input:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]  output:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "input:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]  output:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "input:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]  output:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # B) Encoding id(8)\n",
    "    matrix_size = 8\n",
    "    mlp = MLP(num_inputs=matrix_size, hidden_layers=[10], num_outputs=matrix_size)\n",
    "    \n",
    "    x_mat = np.identity(matrix_size)\n",
    "    x_rows = np.array(x_mat.tolist())\n",
    "    print(\"Matrix id(8): \\n\", x_rows)\n",
    "\n",
    "    # train network\n",
    "    mlp.train(inputs=x_rows, targets=x_rows, epochs=1000, learning_rate=0.5)\n",
    "\n",
    "    # validade model\n",
    "    for row in x_rows:\n",
    "        input = np.array(row)\n",
    "        output = np.round(mlp.forward_propagate(input))\n",
    "        print(\"input: \", input, \" output: \", output)\n",
    "\n",
    "    # B) Encoding Id(15)\n",
    "    matrix_size = 15\n",
    "    mlp = MLP(num_inputs=matrix_size, hidden_layers=[10], num_outputs=matrix_size)\n",
    "    \n",
    "    x_mat = np.identity(matrix_size)\n",
    "    x_rows = np.array(x_mat.tolist())\n",
    "    print(\"Matrix: Id(15)\\n\", x_rows)\n",
    "\n",
    "    # train network\n",
    "    mlp.train(inputs=x_rows, targets=x_rows, epochs=1000, learning_rate=0.5)\n",
    "\n",
    "    # validade model\n",
    "    for row in x_rows:\n",
    "        input = np.array(row)\n",
    "        output = np.round(mlp.forward_propagate(input))\n",
    "        print(\"input: \", input, \" output: \", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
